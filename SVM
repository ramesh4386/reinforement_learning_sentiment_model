# Import required libraries  
import pandas as pd  
import numpy as np  
from sklearn.svm import SVC  
from sklearn.ensemble import BaggingClassifier  
from sklearn.model_selection import train_test_split  
from sklearn.metrics import accuracy_score, precision_score, recall_score  
  
# Load stock market data  
data = pd.read_csv("stock_data.csv")  
  
# Preprocess the data and create binary labels for price trends  
data["PriceTrend"] = np.where(data["PriceAt12"] >= data["OpeningPrice"], 1, 0)  
  
# Split the data into training and test sets  
X = data.drop(["PriceTrend"], axis=1)  
y = data["PriceTrend"]  
  
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  
  
# Build and train the SVM ensemble model using bagging  
svc = SVC(kernel="rbf", C=1, gamma="scale")  
  
bagging_clf = BaggingClassifier(  
    base_estimator=svc,  
    n_estimators=10,  
    max_samples=0.5,  
    bootstrap=True,  
    n_jobs=-1,  
    random_state=42,  
)  
  
bagging_clf.fit(X_train, y_train)  
  
# Make predictions on the test data  
y_pred = bagging_clf.predict(X_test)  
  
# Evaluate the model's performance  
accuracy = accuracy_score(y_test, y_pred)  
precision = precision_score(y_test, y_pred)  
recall = recall_score(y_test, y_pred)  
  
print("Accuracy:", accuracy)  
print("Precision:", precision)  
print("Recall:", recall)  

###################sentiment######################
#pip install transformers  

from transformers import AutoTokenizer, AutoModelForSequenceClassification  
import torch  
  
# Load the pre-trained FinBERT model and tokenizer  
tokenizer = AutoTokenizer.from_pretrained("ProsusAI/finbert")  
model = AutoModelForSequenceClassification.from_pretrained("ProsusAI/finbert")  
  
# Define a function to perform sentiment analysis  
def get_sentiment(text):  
    # Tokenize the input text  
    inputs = tokenizer(text, return_tensors="pt")  
  
    # Get the model's output  
    outputs = model(**inputs)  
  
    # Get the predicted sentiment  
    sentiment = torch.argmax(outputs.logits, dim=1).item()  
    sentiment_map = {0: "negative", 1: "neutral", 2: "positive"}  
  
    return sentiment_map[sentiment]  
  
# Example usage  
text = "The company's stock price surged after the announcement of a new product."  
sentiment = get_sentiment(text)  
print(f"The sentiment of the given text is: {sentiment}")  

################################################################

#!pip install transformers torch

import torch
from transformers import BertTokenizer, BertForSequenceClassification

# Load the ProsusAI/finbert model and tokenizer
model_name = "ProsusAI/finbert"
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(model_name)

# Define a list of financial text data
texts = [
    "The company reported strong earnings for the quarter, which drove the stock price higher.",
    "The economic downturn has negatively impacted the financial markets.",
]

# Tokenize and preprocess the data
inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt", max_length=512)

# Perform sentiment analysis
with torch.no_grad():
    outputs = model(**inputs)
    logits = outputs.logits

# Map logits to sentiment labels based on the model's output structure
predicted_sentiment = "Positive" if logits[0][0] > logits[0][1] else "Negative"

# Print the predicted sentiment
print(f"Predicted Sentiment: {predicted_sentiment}")

###############################################

import pandas as pd
import numpy as np

# Load stock market data
data = pd.read_csv("stock_data.csv")

# Assuming your data has a datetime column "Timestamp," set it as the index
data["Timestamp"] = pd.to_datetime(data["Timestamp"])
data.set_index("Timestamp", inplace=True)

# Calculate the features S4 to S10 for each trading day
data["S4"] = data.groupby(pd.Grouper(freq="D")).apply(lambda x: x["OpeningPrice"].iloc[-1])
data["S5"] = data.groupby(pd.Grouper(freq="D")).apply(lambda x: x["ClosingPrice"].iloc[-1])
data["S6"] = data.groupby(pd.Grouper(freq="D")).apply(lambda x: x["HighestPrice"].max())
data["S7"] = data.groupby(pd.Grouper(freq="D")).apply(lambda x: x["LowestPrice"].min())
data["S8"] = data.groupby(pd.Grouper(freq="D")).apply(lambda x: x["Volume"].sum())

# Calculate S9: The intraday percentage change for each 15-minute interval
data["S9"] = ((data["ClosingPrice"] - data["OpeningPrice"]) / data["OpeningPrice"]) * 100

# Calculate S10: The total transaction amount for each 15-minute interval
data["S10"] = data["ClosingPrice"] * data["Volume"]

# Preprocess the data and create binary labels for price trends
data["PriceTrend"] = np.where(data["PriceAt12"] >= data["S4"], 1, 0)

# Drop rows with missing values resulting from the calculations
data.dropna(inplace=True)

# Select the relevant features for modeling
selected_features = ["S4", "S5", "S6", "S7", "S8", "S9", "S10"]

# Split the data into training and test sets
X = data[selected_features]
y = data["PriceTrend"]

# Now you can proceed with model training and evaluation as previously shown

